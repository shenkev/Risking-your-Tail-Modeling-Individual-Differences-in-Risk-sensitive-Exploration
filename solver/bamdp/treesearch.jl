#=
    Algorithm: we have a map of nodes to V (solved nodes)
    - x and y lists are sorted to to impose permutation invariance
    Each level maintains list of nodes to solve
    Dynamic programming iterates through the list from back to front
    At each level iterates through all nodes
    The computation at each node consists of
    - Figuring out the children
    - Access value of children using map of solved nodes
    - Compute optimal xi‚Äôs and action
    - Update current node‚Äôs value
    Forward pass: build/initialize the list of maps in a front-to-back way
    Solve mdps: solve certainty equivalent mdps at leaves and cache them
    Backward pass: solve value functions
=#

include("./bamdp_structs.jl")
include("../../env/instantiations/uchida_mice_abstraction.jl")
include("../../env/instantiations/constants.jl")
include("./../mdp/util.jl")
include("./../mdp/gpi.jl")

using Match
using .UchidaMdp: construct_mdp
using .GeneralizePolicyIteration: general_policy_iter
using .Util: find_nonzero_idxs, one_hot
using Logging
using DataStructures
using LinearAlgebra:dot
using Distributions: Categorical

logger = ConsoleLogger(stdout)


function forward_pass(v::BAMDPValues)
    push!(v.uhstate_list, Set([UnsortedHyperState(v.s‚ÇÄ)]))
    push!(v.hstate_list, Set([v.s‚ÇÄ]))
    
    for l in range(1, v.L-1)

        push!(v.uhstate_list, Set{UnsortedHyperState}())
        for h in v.uhstate_list[l]
            expand_node(v, h, v.uhstate_list[l+1])
        end

        push!(v.hstate_list, Set([perm_inv(h) for h in v.uhstate_list[l+1]]))
        # @info("Level $(l+1): $(length(v.uhstate_list[l+1])) hyperstates, $(length(v.hstate_list[l+1])) unique")
    end

    v.leaf_istates = Set([InfoState(h.x, dynamic_y_update(h.y, h.œÑ), h.N‚ÇÅ, h.N‚ÇÄ, h.G) for h in v.uhstate_list[end]])
    # @info("$(length(v.leaf_istates)) certainty equivalent MDPs at leaves.")
end


function unpack_soln(soln)
    return soln.mdp, soln.œÑ_transition, soln.info_transition
end


function heuristic_reward_variable(G, r)
    return round(G*r; digits=3)
end


function heuristic_reward(G, r, N‚ÇÅ, N‚ÇÄ)
    return heuristic_reward_variable(G, r) + (N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ))
end


function adjusted_rate(G, Œ≥, r, K)
    # solves ‚àë‚Çñ Œ≥·µè c = ‚àë‚Çñ G‚Çñ r·µ¢ Œ≥·µè  for c where G‚Çñ = G‚ÇÄ (1-r·µ¢)·µè
    # this gives a reward rate s.t. you consume entire pool G in cert_equiv_horizon steps at rate c
    if K > 100
        c = r*G*(1-Œ≥)/(1-(1-r)*Œ≥)
    else
        Œ≥‚Å∫ = Œ≥*(1-r)
        c = r*G*( (Œ≥-1)/(Œ≥‚Å∫-1) ) * ( (Œ≥‚Å∫^(K+1) - 1)/(Œ≥^(K+1) - 1) )
    end

    return round(c; digits=3)
end


function certainty_equiv_params(p, G, N‚ÇÅ, N‚ÇÄ)

    mode = p.cert_equiv
    Œ≥, r‚ÇÅ, r‚ÇÇ, K, k = p.Œ≥, p.r‚ÇÅ, p.r‚ÇÇ, p.cert_equiv_horizon, p.cert_equiv_scale

    if mode == "leftover"
        confident_reward = heuristic_reward_variable(G, r‚ÇÇ) + N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ)
        cautious_reward = heuristic_reward_variable(G, r‚ÇÅ) + N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ)
        return confident_reward, confident_reward - cautious_reward

    elseif mode == "adjusted"
        if K == 0
            K = ceil(Int, log(0.1)/log(1-r‚ÇÇ))
        end
         
        confident_reward = adjusted_rate(G, Œ≥, r‚ÇÇ, K) + N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ)
        cautious_reward = adjusted_rate(G, Œ≥, r‚ÇÅ, K) + N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ)
        return confident_reward, confident_reward - cautious_reward

    elseif mode =="explicit_k"
        confident_reward = k*heuristic_reward_variable(G, r‚ÇÇ) + N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ)
        cautious_reward = k*heuristic_reward_variable(G, r‚ÇÅ) + N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ)
        return confident_reward, confident_reward - cautious_reward

    else
        return N‚ÇÅ/(N‚ÇÅ+N‚ÇÄ), 0.0
    end
end


function reward_pool_update(s, G, r‚ÇÅ, r‚ÇÇ)
    if s == REWARD_STATE
        return G-heuristic_reward_variable(G, r‚ÇÇ)
    elseif s == CAUTIOUS_REWARD_STATE
        return G-heuristic_reward_variable(G, r‚ÇÅ)
    else
        return G
    end
end


function add_heuristic_bonus(R, h, p)
    RÃÇ = copy(R)
    RÃÇ[REWARD_STATE] += heuristic_reward(h.G, p.r‚ÇÇ, h.N‚ÇÅ, h.N‚ÇÄ)
    RÃÇ[CAUTIOUS_REWARD_STATE] += heuristic_reward(h.G, p.r‚ÇÅ, h.N‚ÇÅ, h.N‚ÇÄ)
    return RÃÇ
end


function reward_forgetting(p, G)
    G‚ÇÄ = p.G
    f·µ£ = p.f·µ£
    r_forget_type = p.r_forget_type

    if r_forget_type == "linear"
        return G + f·µ£ <= G‚ÇÄ ? G + f·µ£ : G
    elseif r_forget_type == "exponential"
        return G + f·µ£*(G‚ÇÄ-G)
    else
        throw(ArgumentError("Invalid reward forgetting type: $r_forget_type."))
    end

end


function next_hyperstates(soln, p, h, a)
    mdp, œÑ_transition, info_transition = unpack_soln(soln)
    T_sÃÇ = mdp.T[h.œÑ, h.s, a, :]
    s‚Éó_next = find_nonzero_idxs(T_sÃÇ)
    T_next = T_sÃÇ[s‚Éó_next]
    œÑ_next = œÑ_transition(h.s, s‚Éó_next, h.œÑ)
    x_next, y_next = info_transition(h.s, a, s‚Éó_next, h.x, h.y, h.œÑ)
    G_next = fill(reward_pool_update(h.s, h.G, p.r‚ÇÅ, p.r‚ÇÇ), length(s‚Éó_next))
    return s‚Éó_next, T_next, œÑ_next, x_next, y_next, G_next
end


function expand_node(v::BAMDPValues, h::UnsortedHyperState, h_set)
 
    possible_actions = v.dummy_mdp.possible_actions

    for a in possible_actions(h.s)
        # TODO: implement Œ± updating for pCVaR
        s‚Éó_next, T_next, œÑ_next, x_next, y_next, G_next = next_hyperstates(v.dummy_mdp, v.params, h, a)

        for (s‚Çä, œÑ‚Çä, x‚Çä, y‚Çä, G‚Çä) in zip(s‚Éó_next, œÑ_next, x_next, y_next, G_next)

            if s‚Çä == DEAD_STATE
                continue
            end

            h‚Çä = UnsortedHyperState(s‚Çä, h.Œ±, œÑ‚Çä, h.l+1, x‚Çä, y‚Çä, h.N‚ÇÅ, h.N‚ÇÄ, G‚Çä)
            push!(h_set, h‚Çä)
        end
    end

end


function get_œâ(haz_type, x, y, p)

    h = p.hazard_priors

    œâ = @match haz_type begin
        "weibpostmean" => [x, y, h.Œ±, h.Œ≤, h.k]
        "betapostmean" => [x, y, h.Œ±, h.Œ≤]
        "betamonopostmean" => [x, y, h.Œ±, h.Œ≤]
        "noisyorpostmean" => [x, y, h.Œ±, h.Œ≤]
        _     => throw(ArgumentError("Invalid hazard type: $haz_type."))
    end

    return œâ
end


function solve_certain_mdps(v::BAMDPValues, Œ±_only::Float64)
    p = v.params
    haz_type = p.hazard_type

    for i in collect(v.leaf_istates)

        r‚ÇÄ, C‚Çì = certainty_equiv_params(p, i.G, i.N‚ÇÅ, i.N‚ÇÄ)

        mdp = construct_mdp(haz_type=haz_type, œâ=get_œâ(haz_type, i.x, i.y, p),
        detect_cost=p.C‚Çí, retreat_cost=p.C·µ£, dying_cost=p.C‚Çú, Œ≥=p.Œ≥, Œ±‚Éó=p.Œ±‚Éó, H=p.H,
        p‚ÇÅ=p.p‚ÇÅ, p‚ÇÇ=p.p‚ÇÇ, caution_cost=C‚Çì, r‚ÇÄ=r‚ÇÄ, converge_thresh=p.Œî)

        general_policy_iter(mdp, "value_iter", v.params.I, "n", Œ±_only)
        v.leaf_mdps[i] = mdp.soln
    end

end


function solve_last_level(v::BAMDPValues)
    for h in v.hstate_list[end]
        soln = v.leaf_mdps[InfoState(h.x, dynamic_y_update(h.y, h.œÑ), h.N‚ÇÅ, h.N‚ÇÄ, h.G)]

        i‚Çê = findfirst(x->x==h.Œ±, soln.values.Œ±‚Éó)

        if isnothing(i‚Çê)
            throw(ErrorException("Œ±=$(h.Œ±) not in Œ±‚Éó of certainty solution. Interpolation isn't implemented."))
        end

        v.ùïç[h] = soln.values.ùïç[i‚Çê, h.œÑ, h.s]  
    end
end


function optimize_ncvar(input, Œ±)
    cdf = 0.0
    sorted_by_value = sort(input, by=first)
    Œæ = []

    for (V, p) in sorted_by_value
        if cdf == 1
            push!(Œæ, 0)
        elseif cdf + p/Œ± > 1
            push!(Œæ, (1-cdf)/p)
            cdf = 1
        else
            push!(Œæ, 1/Œ±)
            cdf += p/Œ±
        end
    end

    V = dot(Œæ, [x[1]*x[2] for x in sorted_by_value])
    # invert sorting to restore original indexing
    idxs = sortperm(sortperm(input, by=first))
    return [Œæ[i] for i in idxs], V
end


function nCVaR_Bellman(s, Œ±, R, Œ≥, T_next, V_next)
    # Boundary Conditions
    if Œ± == 0  # minimum over next state V at Œ±=0

        V_min = minimum(V_next)
        V = R[s] + Œ≥*V_min
        Œæ‚Éó = zeros(size(T_next))
        s‚Éó_min = [i for (i,V) in enumerate(V_next) if V==V_min]

        for i in s‚Éó_min
            Œæ‚Éó[i] = 1/(T_next[i]*length(s‚Éó_min))  # this gives uniform T' = Œæ‚Éó*T over min next states            
        end
        
        return V, Œæ‚Éó
    else
        # Non-Boundary Case
        Œæ‚Éó, V = optimize_ncvar([(V_next[i], T_next[i]) for i in range(1, length(V_next))], Œ±)
        V = R[s] + Œ≥*V

        return V, Œæ‚Éó
    end
end


function inference_mdp(v, h)
    p = v.params
    haz_type = p.hazard_type

    return construct_mdp(haz_type=haz_type, œâ=get_œâ(haz_type, h.x, dynamic_y_update(h.y, h.œÑ), p), 
     detect_cost=p.C‚Çí, retreat_cost=p.C·µ£,
     dying_cost=p.C‚Çú, Œ≥=p.Œ≥, Œ±‚Éó=p.Œ±‚Éó, H=p.H, p‚ÇÅ=p.p‚ÇÅ, p‚ÇÇ=p.p‚ÇÇ, caution_cost=p.C‚Çì,
     r‚ÇÄ=0.0, converge_thresh=p.Œî).soln
end


function solve_node(v::BAMDPValues, h)
    if haskey(v.ùïç, h)
        return v.ùïç[h]
    end

    soln = inference_mdp(v, h)
    mdp, possible_actions = soln.mdp, soln.possible_actions
    R = add_heuristic_bonus(mdp.R, h, v.params)

    runs_a = []

    for a in range(1, mdp.A)
        if a in possible_actions(h.s)
            s‚Éó_next, T_next, œÑ_next, x_next, y_next, G_next = next_hyperstates(soln, v.params, h, a)
            V_next = [s‚Çä == DEAD_STATE ? 0.0 : v.ùïç[HyperState(s‚Çä, h.Œ±, œÑ‚Çä, h.l+1, x‚Çä, y‚Çä, h.N‚ÇÅ, h.N‚ÇÄ, G‚Çä)] for (s‚Çä, œÑ‚Çä, x‚Çä, y‚Çä, G‚Çä) in zip(s‚Éó_next, œÑ_next, x_next, y_next, G_next)]
            V, Œæ‚Éó = nCVaR_Bellman(h.s, h.Œ±, R, v.params.Œ≥, T_next, V_next)
            push!(runs_a, (V, Œæ‚Éó, s‚Éó_next))
        else
            push!(runs_a, (-Inf, -Inf, -Inf))  # stupid hack to deal with illegal actions       
        end
    end

    # compute max action
    Qs = [run[1] for run in runs_a]
    a_max = argmax(Qs)
    Q_max, Œæ‚Éó, s‚Éó_next = runs_a[a_max]

    # update values
    v.‚Ñö[h] = Qs
    v.ùïç[h] = Q_max
    Œæ_all = zeros(mdp.S)

    for i in range(1, length(s‚Éó_next))
        Œæ_all[s‚Éó_next[i]] = Œæ‚Éó[i]
    end

    v.Œæ·µ•[h] = Œæ_all

   # update policy
   a‚Éó_best = convert(Array{Float64}, abs.(Qs .- maximum(Qs)) .< 1e-3)  # 1e-3 arbitrary small number
   v.œÄ[h] = sum(a‚Éó_best) == 1 ? one_hot(a_max, mdp.A) : a‚Éó_best ./ sum(a‚Éó_best)
end


function backward_pass(v::BAMDPValues, Œ±_only, mthread=false)
    if mthread
        solve_certain_mdps(v, Œ±_only, "./out/leaf_mdps.jls")  # calls method in run.jl which is hacky
    else
        solve_certain_mdps(v, Œ±_only)
    end

    solve_last_level(v)

    for i in reverse(1:v.L-1)
        for h in v.hstate_list[i]
            solve_node(v, h)
        end
    end

end


function plan(p, s‚ÇÄ, Œ±_only=nothing, mthread=false)

    v = BAMDPValues(p, s‚ÇÄ, p.L, 
        construct_mdp(haz_type="uniform", œâ=[], detect_cost=-1.5, retreat_cost=-1.5, dying_cost=-5,
         Œ≥=0.9, Œ±‚Éó=p.Œ±‚Éó, H=p.H, p‚ÇÅ=0.4, p‚ÇÇ=0.6, caution_cost=0.0, r‚ÇÄ=0.0, converge_thresh=1e-4))

    forward_pass(v)
    backward_pass(v, Œ±_only, mthread)
    return v
end


function online_simulation(p; Œª_true)

    Œ± = p.Œ±‚Éó[1]
    h‚ÇÄ = HyperState(1, Œ±, 1, 1, [], [], p.N‚ÇÅ, p.N‚ÇÄ, p.G)

    soln = construct_mdp(haz_type="weibull", œâ=[2.0, Œª_true^2.0],
     detect_cost=p.C‚Çí, retreat_cost=p.C·µ£, dying_cost=p.C‚Çú, Œ≥=p.Œ≥, Œ±‚Éó=p.Œ±‚Éó, H=p.H,
     p‚ÇÅ=p.p‚ÇÅ, p‚ÇÇ=p.p‚ÇÇ, caution_cost=p.C‚Çì, r‚ÇÄ=0.0, converge_thresh=p.Œî).soln

    mdp, œÑ_transition, info_transition = unpack_soln(soln)
    T, R = mdp.T, mdp.R

    trajectory = [h‚ÇÄ]
    rewards = []
    actions = []

    function forgetting_update(v, s‚ÇÄ)
        
        G = reward_forgetting(v.params, s‚ÇÄ.G)
        return HyperState(s‚ÇÄ.s, s‚ÇÄ.Œ±, s‚ÇÄ.œÑ, s‚ÇÄ.l, s‚ÇÄ.x, s‚ÇÄ.y, s‚ÇÄ.N‚ÇÅ, s‚ÇÄ.N‚ÇÄ, G)
    end
        
    function true_dynamics_update(v, s‚ÇÄ)
        push!(rewards, R[s‚ÇÄ.s])

        max_œÑ = size(T)[1]
        if s‚ÇÄ.œÑ == max_œÑ
            a = LEAVE_ACTION
        else
            a = argmax(v.œÄ[s‚ÇÄ])
        end

        if s‚ÇÄ.s == DEAD_STATE || length(trajectory) >= p.max_steps
            return "Early Termination"
        end

        push!(actions, a)
        
        T_sÃÇ = T[s‚ÇÄ.œÑ, s‚ÇÄ.s, a, :]
        sÃÇ = rand(Categorical(T_sÃÇ))  # sample

        # todo: hack, make sure we don't get detected
        if sÃÇ == DETECT_STATE
            sÃÇ = REWARD_STATE
        elseif sÃÇ == CAUTIOUS_DETECT_STATE
            sÃÇ = CAUTIOUS_REWARD_STATE
        end
        
        œÑ = œÑ_transition(s‚ÇÄ.s, [sÃÇ], s‚ÇÄ.œÑ)[1]
        x, y = info_transition(s‚ÇÄ.s, a, [sÃÇ], s‚ÇÄ.x, s‚ÇÄ.y, s‚ÇÄ.œÑ)
        N‚ÇÅ = s‚ÇÄ.N‚ÇÅ; N‚ÇÄ = s‚ÇÄ.N‚ÇÄ+1
        G = reward_pool_update(s‚ÇÄ.s, s‚ÇÄ.G, v.params.r‚ÇÅ, v.params.r‚ÇÇ)
        return HyperState(sÃÇ, Œ±, œÑ, 1, x[1], y[1], N‚ÇÅ, N‚ÇÄ, G)
    end

    while true
        h‚ÇÄ = trajectory[end]
        v = plan(p, h‚ÇÄ, Œ±)
        h‚Å∫ = true_dynamics_update(v, h‚ÇÄ)

        if h‚Å∫ == "Early Termination"
            break
        end

        h‚Å∫ = forgetting_update(v, h‚Å∫)

        push!(trajectory, h‚Å∫)
    end
    
    return (trajectory, rewards, actions)
end